# Generative model 

## Class structure

The class structure of these modules maps roughly onto the structure of our generative model. 

Following Figure 2B, a scene is defined as a collection of sources - as represented by the `Scene` class.
This class also contains a cochleagram `C_scene`, which is generated by summing the sound produced by each of its sources (bottom of Figure 2B).

```
Scene
  n_sources: int
  sources: List[Source]
  C_scene: array[float]
```

Each source has a sound type: noise, harmonic or whistle.
This determines the kind of sound it will eventually render, as seen in the difference in the 'Source sounds' in Figure 2B.
Rendering methods are defined in `bass.render`.

Sources of different types have different variables.
For example, a harmonic source will have a fundamental frequency but a noise source will not.

Our `Source` class just defines the kinds of variables a source contains.
The various 'source parameters' depicted in the 'Sources' panel of Figure 2B are split up two classes:
`Sequence` contains those related to event timings (onsets and offsets), while `gps` contains those related to the amplitude, f0 and spectra.
These classes are described below.

```
Source
  source_type: str ("noise", "harmonic" or "whistle")
  sequence: Sequence
  gps["amplitude"]: GP 
  gps["f0"]: GP (if harmonic or whistle)
  gps["spectrum"]: GP (if noise or harmonic)
  renderer: Renderer
```

A `Sequence` is made up of a list of `Event`s, each with their own `onset` and `offset`.
These timings are distributed according to the source parameters `mu` and `precision`.
The source parameters of `Sequence` correspond to the knobs with red labels ('Rest' and 'Active' in Figure 2B). 
The `Event` onset and offset intervals correspond to the 'Active/Rest' box in the 'Events' panel of Figure 2B. 

```
Sequence
  n_events: int
  events: List[Event]
  # Source parameters
	gap.precision: float (see SequenceSourcePrior)
  gap.mu: float
	duration.precision: float (see SequenceSourcePrior)
  duration.mu: float

Event
  # Event variables
  onset.interval: float
  offset.interval: float
```

The `GP` class is similar to the `Sequence` class in that it contains source parameters and the corresponding event variables. 
The source parameters of `GP` correspond to the knobs with blue/purple labels ('Amplitude', 'Frequency', 'Spectrum').
`GP.feature.y` corresponds to the trajectories drawn in the 'Events' panel of Figure 2B.
For example, for an amplitude GP, `feature.y` is a vector representing the sampled amplitude at every moment in time.

```
GP
  # Source parameters
  mean_module.mu: float (see SampledConstantMean, ZeroMean) 
  covar_module.sigma: float (see ScaleSquaredExponential, Matern, ScaleTemporalSquaredExponential) 
  covar_module.scale: float
  # Event variables
  feature.y: array[float]
```

### Fitting source priors
We also define classes for fitting the source priors to recorded audio (see section A.3, _Generative model: source priors_). The source prior distributions are determined by simultaneously inferring several scenes from several observed sounds at once, which gives a distribution over source parameters.
```
Scenes
  source_type: str ("noise", "harmonic" or "whistle")
  sequence_hp: Dict[float] 
  feature_hp: Dict[float] 
  scenes: List[SharedScene]
```
`SharedScene` has a structure equivalent to `Scene`. The differences are that:
- The meta-source parameters that define the source priors are now random variables (i.e., `torch.nn.Parameter`s), defined in `Scenes.sequence_hp` and `Scenes.feature_hp`, instead of being fixed values in the config file.
- Those meta-source parameters are shared across all `SharedScene`s


## Main class methods

Our model integrates code for stochastic variational inference (section B.1.1, _Inference: Hypothesis optimization and scene selection_). 

Specifically, every class in the model implements the following two methods:
- `log_q`: This method samples all latent variables (such as `feature.y`) and returns their log probability under the variational distribution ($q$) from which they were sampled
- `log_p`: This methods returns the log probability of the all latent variables under the prior distribution ($p$)

`Scene` also implements `log_likelihood`, which compares the rendered scene sound with the observed sound. This allows us to compute the evidence lower bound (ELBO), which is maximized in variational inference.

Every class in the model also implements the following two constructor methods:
- `sample`: creates an instance by sampling from the prior (see Figure A.2). This function is called to create training data for the event proposal network.
- `hypothesize`: creates an instance from a specific description of the sources and events that make up a scene. This function is called:
  1. in enumerative inference, for directly defining a scene of interest in a psychophysics experiment
  2. in sequential inference, for building up a scene through constructing sources out of candidate events 

## Truncated normal

Get truncated normal from `https://github.com/toshas/torch_truncnorm`. 